---
title: Compute-Optimal Scaling for Value-Based Deep RL
link: https://arxiv.org/abs/2508.14881
date: 2025-09-02
highlight: true
image_before: /assets/images/model_scaling_before.png
image_after: /assets/images/model_scaling_3d.mp4
---

**Preston Fu**,
[Oleh Rybkin](https://olehrybkin.com/),
[Zhiyuan Zhou](https://zhouzypaul.github.io/).
[Michal Nauman](https://scholar.google.com/citations?user=GnEVRtQAAAAJ&hl=en),
[Pieter Abbeel](https://people.eecs.berkeley.edu/~pabbeel/),
[Sergey Levine](https://people.eecs.berkeley.edu/~svlevine/),
[Aviral Kumar](https://aviralkumar2907.github.io/)

[arXiv](https://arxiv.org/abs/2508.14881) / [Thread](https://x.com/preston_fu/status/1962920781387882841)

We analyze the interplay of model size, UTD, and batch size. For
small models, reducing the training error by increasing the batch size worsens
generalization; we trace this effect to poor-quality TD-targets.
Leveraging this, we fit the best batch size and derive and verify the
budget-optimal data-compute tradeoff.
